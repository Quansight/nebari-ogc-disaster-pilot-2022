{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff2e093-ceac-4901-9f23-3afa16c6f3d4",
   "metadata": {},
   "source": [
    "# Working with big data using Dask\n",
    "\n",
    "Working with large datasets can pose a few challenges, running into frequent memory issues is a common issue. \n",
    "[Dask](https://docs.dask.org/en/stable/) is a free and open-source library for parallel computing in Python,\n",
    "enabling data scientist and others to use their favorite PyData tools at scale.\n",
    "\n",
    "## Dask integration on Nebari\n",
    "\n",
    "Nebari uses [Dask Gateway](https://gateway.dask.org/) to expose auto-scaling compute clusters automatically \n",
    "configured for the user, and provides a secure way to managing dask clusters. \n",
    "\n",
    "<details>\n",
    "<summary> Want more information on how that works? </summary>\n",
    "\n",
    "Dask consists of 3 main components `client`, `scheduler` and `workers`.\n",
    "- The end users interact with the `client`. \n",
    "- The `scheduler` tracks metrics and coordinate workers.\n",
    "- The `workers` are the threads/processes that executes computations.\n",
    "\n",
    "The `client` interacts with both `scheduler` (sends instructions) and `workers` (collects results)\n",
    "\n",
    "Check out the [Dask Gateway documentation](https://gateway.dask.org/) for a full explanation.\n",
    "\n",
    "</details>\n",
    "\n",
    "## Setting up Dask Gateway\n",
    "\n",
    "We will start by creating a Jupyter notebook. Select an environment from the `Select kernel` dropdown menu \n",
    "(located on the top right of your notebook). Be sure to select an environment which incudes `Dask`!\n",
    "\n",
    "Nebari has set of pre-defined options for configuring the Dask profiles that we have access to. These can be \n",
    "accessed via Dask Gateway options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce8d9c57-2057-40b2-a82b-84db4ae4bf04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82bce1297c484a82bf14b7cfc0f3626a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>Cluster Options</h2>'), GridBox(children=(HTML(value=\"<p style='font-weight: bo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dask_gateway import Gateway\n",
    "# instantiate dask gateway\n",
    "gateway = Gateway()\n",
    "\n",
    "# view the cluster options UI\n",
    "options = gateway.cluster_options()\n",
    "options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f0f6a4-a8a8-4138-8e27-5f82e1152606",
   "metadata": {},
   "source": [
    "Using the `Cluster Options` interface, we can specify the conda environment, the instance type, and any additional \n",
    "environment variables we'll need. \n",
    "\n",
    ":::warning\n",
    "It’s important that the environment used for your notebook matches the Dask worker environment!\n",
    "\n",
    "The Dask worker environment is specified in your deployment directory under `/image/dask-worker/environment.yaml`\n",
    ":::\n",
    "\n",
    "## Creating a Dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2029312-916d-416a-b341-272045ee4f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1942fc8ee3c44be8831d85bcf51ce71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>GatewayCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n<style scoped>\\n    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a new cluster with our options\n",
    "cluster = gateway.new_cluster(options)\n",
    "# view the cluster UI\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add2c021-5fd4-46b0-8c0f-a4668f55628d",
   "metadata": {},
   "source": [
    "We have the option to choose between `Manual Scaling` and `Adaptive Scaling`.\n",
    "\n",
    "If you know the resources that would be required for the computation, you can select `Manual Scaling` and \n",
    "define a number of workers to spin up. These will remain in the cluster until it is shut down. \n",
    "\n",
    "Alternatively, if you aren't sure how many workers you'll need, or if parts of your workflow require more workers\n",
    "than others, you can select `Adaptive Scaling`. Dask Gateway will automatically scale the number of workers\n",
    "(spinning up new workers or shutting down unused ones) depending on the computational burder. `Adaptive Scaling` is\n",
    "a safe way to prevent running out of memory, while not burning excess resources. \n",
    "\n",
    "You may also notice a link to the Dask Dashboard in this interface. We'll discuss this in a later section. \n",
    "\n",
    "## Viewing the Dask Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347d8371-d22b-4e76-9e87-91894a20bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the client for the cluster\n",
    "client = cluster.get_client()\n",
    "# view the client UI\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2d0f00-49a5-4b0c-b277-2d21fd4ae307",
   "metadata": {},
   "source": [
    "The `Dask Client` interface gives us a brief summary of everything we've set up so far. \n",
    "\n",
    "## Now for the fun part - let's code with Dask! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23375202-ac4e-42ce-b21f-6800819061b8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m x\n\u001b[1;32m      4\u001b[0m z \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/conda/filesystem/b120fefa11a29a0bec308fa06888ec19777904604b7baeac7880544e7d51fe08-20220707-012624-515057-4-ogc/lib/python3.9/site-packages/dask/base.py:292\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    dask.base.compute\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/home/conda/filesystem/b120fefa11a29a0bec308fa06888ec19777904604b7baeac7880544e7d51fe08-20220707-012624-515057-4-ogc/lib/python3.9/site-packages/dask/base.py:575\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m     keys\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_keys__())\n\u001b[1;32m    573\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 575\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/home/conda/filesystem/b120fefa11a29a0bec308fa06888ec19777904604b7baeac7880544e7d51fe08-20220707-012624-515057-4-ogc/lib/python3.9/site-packages/dask/threaded.py:81\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, result, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pool, multiprocessing\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mPool):\n\u001b[1;32m     79\u001b[0m         pool \u001b[38;5;241m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 81\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mget_async\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_thread_get_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpack_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpack_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m/home/conda/filesystem/b120fefa11a29a0bec308fa06888ec19777904604b7baeac7880544e7d51fe08-20220707-012624-515057-4-ogc/lib/python3.9/site-packages/dask/local.py:497\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwaiting\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mready\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunning\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    496\u001b[0m     fire_tasks(chunksize)\n\u001b[0;32m--> 497\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, res_info, failed \u001b[38;5;129;01min\u001b[39;00m \u001b[43mqueue_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mresult():\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m    499\u001b[0m             exc, tb \u001b[38;5;241m=\u001b[39m loads(res_info)\n",
      "File \u001b[0;32m/home/conda/filesystem/b120fefa11a29a0bec308fa06888ec19777904604b7baeac7880544e7d51fe08-20220707-012624-515057-4-ogc/lib/python3.9/site-packages/dask/local.py:134\u001b[0m, in \u001b[0;36mqueue_get\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mqueue_get\u001b[39m(q):\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/conda/filesystem/b120fefa11a29a0bec308fa06888ec19777904604b7baeac7880544e7d51fe08-20220707-012624-515057-4-ogc/lib/python3.9/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/conda/filesystem/b120fefa11a29a0bec308fa06888ec19777904604b7baeac7880544e7d51fe08-20220707-012624-515057-4-ogc/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import dask.array as da\n",
    "x = da.random.random((100000, 100000), chunks=(1000, 1000))\n",
    "y = x * x\n",
    "z = y.mean(axis=1)\n",
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6d21c8-9a92-4f02-930a-a50bcff203fd",
   "metadata": {},
   "source": [
    "In the above code snippet, we are first generating a random array of shape (10000*10000), which is a large array.\n",
    "In order to fit it into our memory we specify the argument `chunks` which breaks the underlying array into\n",
    "chunks. Here we are using uniform dimension `1000`, meaning chunks of 1000 in each dimension. Storing it in variable\n",
    "`x`. Further some simple computations are performed, and finally we compute the column wise mean operation \n",
    "on the array `z`.\n",
    "\n",
    "### Dask diagnostic UI\n",
    "\n",
    "Dask comes with an inbuilt dashboard containing multiple plots and tables containing live information as \n",
    "the data gets processed. Let's understand the dashboard plots `Task Stream` and `Progress`. \n",
    "The colours and the interpretation would differ based on the computation we choose.\n",
    "\n",
    "Each of the computation in split into multiple tasks for parallel execution. From the progress bar we see 04\n",
    "distinct colours associated with different computation. Under task stream (a streaming plot) each row represents a thread\n",
    "and the small rectangles within are the individual tasks. The tiny white spaces shows that the worker was ideal during \n",
    "that period of time.\n",
    "\n",
    "![dask diagnostic UI](./img/dask_diagostic_UI.png)\n",
    "\n",
    "### Viewing the dashboard inside of JupyterLab\n",
    "\n",
    "[Dask-labextension](https://github.com/dask/dask-labextension) provides a JupyterLab extension to manage Dask clusters,\n",
    "as well as embed Dask's dashboard plots directly into JupyterLab panes.\n",
    "Nebari includes this extension by default, elevating the overall developer experience.\n",
    "\n",
    "![Dask-labextension ui](./img/dask_labextension.png)\n",
    "\n",
    "### Shutting down the cluster\n",
    "\n",
    "As you you may have noticed, its easy to spin up a lot of compute, really quickly. \n",
    "\n",
    "**With Great Power Comes Great Responsibility**\n",
    "\n",
    "**ALWAYS** remember to shutdown your cluster!! *Resources cost something!!* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d289474c-ae42-4619-9a37-5d52b7792683",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close(shutdown=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2af348-1f08-4bcb-a39d-9f22dbbdc8a9",
   "metadata": {},
   "source": [
    "## Using Dask safely\n",
    "\n",
    "If you're anything like us, we've forgotten to shut down our cluster a time or two. Wrapping the dask-gateway in a \n",
    "context manager is a great practice that ensures the cluster is fully shutdown once the task is complete! \n",
    "\n",
    "### Sample Dask `context manager` configuration\n",
    "\n",
    "We like to use something like this context manager to help us manager our clusters. It can be written once and \n",
    "included in your codebase. The one we've included here includes some default setup options. You can write your \n",
    "own to adjust to your needs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fb9123-9657-42c9-97c1-53241f63a627",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "import time\n",
    "import dask.array as da\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import dask\n",
    "from distributed import Client\n",
    "from dask_gateway import Gateway\n",
    "\n",
    "@contextmanager\n",
    "def dask_cluster(n_workers=2, worker_type=\"Small Worker\", conda_env=\"filesystem/dask\"):\n",
    "    try:\n",
    "        gateway = Gateway()\n",
    "        options = gateway.cluster_options()\n",
    "        options.conda_environment = conda_env\n",
    "        options.profile = worker_type\n",
    "        print(f\"Gateway: {gateway}\")\n",
    "        for key, value in options.items():\n",
    "            print(f\"{key} : {value}\")\n",
    "\n",
    "        cluster = gateway.new_cluster(options)\n",
    "        client = Client(cluster)\n",
    "        if os.getenv(\"JUPYTERHUB_SERVICE_PREFIX\"):\n",
    "            print(cluster.dashboard_link)\n",
    "\n",
    "        cluster.scale(n_workers)\n",
    "        client.wait_for_workers(1)\n",
    "\n",
    "        yield client\n",
    "\n",
    "    finally:\n",
    "        cluster.close()\n",
    "        client.close()\n",
    "        del client\n",
    "        del cluster\n",
    "```\n",
    "\n",
    "Now we can write our compute tasks inside of the context manager and all of the setup and teardown\n",
    "is managed for us! \n",
    "\n",
    "```python\n",
    "with dask_cluster() as client:\n",
    "    x = da.random.random((10000, 10000), chunks=(1000, 1000))\n",
    "    y = x + x.T\n",
    "    z = y[::2, 5000:].mean(axis=1)\n",
    "    result = z.compute()\n",
    "    print(client.run(os.getpid))\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Kudos ✨, we now have a working Dask cluster inside Nebari.  \n",
    "Now go load up your own big data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75b7891-f098-4f70-8c39-f7b5bda2b445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "filesystem-ogc",
   "language": "python",
   "name": "conda-env-filesystem-ogc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
